{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBxPnCcuuAuO"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 DeepMind Technologies Limited. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "metadata": {
        "id": "x8Qd6mWRuFHz"
      },
      "cell_type": "markdown",
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/genai-processors/blob/main/notebooks/function_calling_with_gemma.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "A8yIcf81uf_s"
      },
      "cell_type": "markdown",
      "source": [
        "# Function Calling with FunctionGemma ü§ñ\n",
        "\n",
        "This notebook demonstrates how to use the GenAI Processor library to implement a\n",
        "Function Calling loop with the FunctionGemma model (via Hugging Face\n",
        "transformers). To ensure safety across all string inputs, both function calling\n",
        "and response parsing are executed at the token ID level.\n",
        "\n",
        ""
      ]
    },
    {
      "metadata": {
        "id": "rYqU_6n4v-dQ"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. üõ†Ô∏è Setup\n",
        "\n",
        "First, install the GenAI Processors library:"
      ]
    },
    {
      "metadata": {
        "id": "bDal7EHPwCDk"
      },
      "cell_type": "code",
      "source": [
        "!pip install genai-processors"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we are going to use FunctionGemma 270m, you have to accept the terms of use\n",
        "for Gemma. You can accept the license on Hugging Face by clicking on the Agree\n",
        "and access repository button on the model page at:\n",
        "http://huggingface.co/google/functiongemma-270m-it.\n",
        "\n",
        "After you have accepted the license, you need a valid Hugging Face Token to\n",
        "access the model. If you are running inside a Google Colab, you can securely use\n",
        "your Hugging Face Token using the Colab secrets otherwise you can set the token\n",
        "as directly in the login method."
      ],
      "metadata": {
        "id": "xI4JChGLTNI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Login into Hugging Face Hub\n",
        "hf_token = userdata.get('HF_TOKEN')  # If you are running inside a Google Colab\n",
        "login(hf_token)"
      ],
      "metadata": {
        "id": "eHd0A_euTSWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, import the required modules and finish setup."
      ],
      "metadata": {
        "id": "gAIVSq-XToGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from genai_processors import content_api\n",
        "from genai_processors import streams\n",
        "from genai_processors.core import function_calling\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()  # Needed to run async loops in Colab\n",
        "\n",
        "\n",
        "def print_part(part: content_api.ProcessorPart):\n",
        "  if not part.substream_name:\n",
        "    # default substream - contains what the user would see.\n",
        "    print(f'{part.text}', flush=True, end='')\n",
        "\n",
        "  if part.substream_name:\n",
        "    # subtream_name = \"function_call\" / internal function calls.\n",
        "    if part.function_call:\n",
        "      print(\n",
        "          f'\\033[96m FC: {part.function_call.name}:'\n",
        "          f' {part.function_call.args}\\033[0m ',\n",
        "          flush=True,\n",
        "      )\n",
        "    elif part.function_response:\n",
        "      print(\n",
        "          f'\\033[96m FR: {part.function_response.response}\\033[0m ',\n",
        "          flush=True,\n",
        "      )\n",
        "    else:\n",
        "      print(f'\\033[96m {part}\\033[0m ', flush=True, end='')"
      ],
      "metadata": {
        "id": "5Z154DHsHI28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wWoaB7Xw1I-1"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. üëÜ Define the functions to be called\n",
        "\n",
        "GenAI Processors supports automated function calling, deriving tool definitions\n",
        "directly from your Python function signatures. Simply define your function with\n",
        "a descriptive docstring. While introspection captures the basic schema, a\n",
        "detailed docstring covering arguments and return values is critical for the\n",
        "model to generate accurate calls."
      ]
    },
    {
      "metadata": {
        "id": "GuSwrv5D1pAy"
      },
      "cell_type": "code",
      "source": [
        "def get_temperature_celsius(location: str) -> dict[str, float]:\n",
        "  \"\"\"Gets the temperature in Celsius at location including a weather description.\n",
        "\n",
        "  Args:\n",
        "    location: name of the city, region or place where the weather is requested.\n",
        "\n",
        "  Returns:\n",
        "    The temperature in Celsius.\n",
        "  \"\"\"\n",
        "  return {\"temperature\": 21}\n",
        "\n",
        "\n",
        "def to_fahrenheit(temperature_celsius: float) -> float:\n",
        "  \"\"\"Gets the temperature in Fahrenheit from a temperature in Celsius.\n",
        "\n",
        "  Args:\n",
        "    temperature_celsius: temperature in Celsius.\n",
        "\n",
        "  Returns:\n",
        "    The temperature in Fahrenheit.\n",
        "  \"\"\"\n",
        "  return temperature_celsius * 9 / 5 + 32\n",
        "\n",
        "\n",
        "def get_events(location: str) -> list[str]:\n",
        "  \"\"\"Gets the list of events happening today at a given location.\n",
        "\n",
        "  Args:\n",
        "    location: name of the city, region or place where the events take place.\n",
        "\n",
        "  Returns:\n",
        "    The list of event descriptions.\n",
        "  \"\"\"\n",
        "  return [\"City Hall concert at 8pm\", \"Ice skating show at 10pm\"]\n",
        "\n",
        "\n",
        "# Create the tool list that will be used below for all function calling\n",
        "# processors.\n",
        "tool_list = [get_temperature_celsius, to_fahrenheit, get_events]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "pklvojGQ2_A9"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. ‚ú® Create the Transformer model\n",
        "\n",
        "Configure the parameters in the form below:\n",
        "\n",
        "-   **model_name**: Specify a Hugging Face FunctionGemma model (standard or\n",
        "    fine-tuned). Any model implementing the FunctionGemma function calling\n",
        "    format is compatible.\n",
        "\n",
        "-   **log_chat_template**: Set to True to output the full prompt history to the\n",
        "    runtime logs. While verbose, this is useful for debugging generated function\n",
        "    calls and responses.\n",
        "\n",
        "-   **system_instruction**: use this to guide the model's behavior or refine\n",
        "    specific function calling requirements."
      ]
    },
    {
      "metadata": {
        "id": "8ArjWNMB3Nyn",
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "from genai_processors.core import transformers_model\n",
        "\n",
        "model_name = 'google/functiongemma-270m-it'  # @param {type: \"string\"}\n",
        "log_chat_template = True  # @param {\"type\":\"boolean\"}\n",
        "system_instruction = 'You are a model that can do function calling with the following functions.'  # @param {type: \"string\"}"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE**: unless you do fine-tuning on the functions you plan to use, it is\n",
        "highly recommended to end the system instructions with the sentence \"You are a\n",
        "model that can do function calling with the following functions\"."
      ],
      "metadata": {
        "id": "Y-sFgmr-tYUG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now create the transformer model: you only need to add the functions\n",
        "you'd like to use in the constructor as shown below."
      ],
      "metadata": {
        "id": "K5rwuHImcrfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_model = transformers_model.TransformersModel(\n",
        "    model_name=model_name,\n",
        "    generate_content_config=transformers_model.GenerateContentConfig(\n",
        "        tools=tool_list,\n",
        "        system_instruction=system_instruction,\n",
        "    ),\n",
        "    log_chat_template=log_chat_template,\n",
        "    tool_response_format='dict',\n",
        ")"
      ],
      "metadata": {
        "id": "lzdKNsZdtU7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. üì¶ Wrap the transformer model into a Function Calling loop\n",
        "\n",
        "Adding automatic function calling to an existing transformer implementing the\n",
        "FunctionGemma function calling format is done as follows:"
      ],
      "metadata": {
        "id": "6sz0xGkFdtUD"
      }
    },
    {
      "metadata": {
        "id": "UC3jcXXE4IMP"
      },
      "cell_type": "code",
      "source": [
        "fc = function_calling.FunctionCalling(\n",
        "    hf_model,\n",
        "    fns=tool_list,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we need to add the functions here. Those functions should be the same\n",
        "as the one provided during `hf_model` creation. The `FunctionCalling` processor\n",
        "will handle the function calling loop, the scheduling and execution of functions\n",
        "as well as the function declarations in the prompt."
      ],
      "metadata": {
        "id": "Z8rwlmukt1A2"
      }
    },
    {
      "metadata": {
        "id": "btH0h1Gq47ra"
      },
      "cell_type": "markdown",
      "source": [
        "## 5. ‚ñ∂Ô∏è Run the function calling processor\n",
        "\n",
        "The `FunctionCalling` processor is a standard GenAI Processor and can therefore\n",
        "be used and combined with any other GenAI processor. Here, we typically apply it\n",
        "to a stream of content, that is simply a piece of text.\n",
        "\n",
        "The function calls (FC) and function responses (FR) are shown in cyan."
      ]
    },
    {
      "metadata": {
        "id": "TXDcjJXg5A_i"
      },
      "cell_type": "code",
      "source": [
        "input_stream = streams.stream_content(['What is the temperature in London?'])\n",
        "\n",
        "async for part in fc(input_stream):\n",
        "  print_part(part)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is called twice here.\n",
        "\n",
        "*   First call to produce a Function Call (FC) response that is intercepted by\n",
        "    the `function_calling` processor. The `get_temperature_celsius()` function\n",
        "    is executed and function response is returned.\n",
        "*   Second call to gives an answer to the user based on the function response."
      ],
      "metadata": {
        "id": "U-JwxbwsFgae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. üõ§Ô∏è Parallel calls\n",
        "\n",
        "FunctionGemma is currently trained to handle a single step and can be used to\n",
        "call a single function as above or a few functions that can be run in parallel."
      ],
      "metadata": {
        "id": "toDuCd_HgWjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_stream = streams.stream_content([\n",
        "    'What is the temperature in London? Give me also the list of events for'\n",
        "    ' today.'\n",
        "])\n",
        "\n",
        "async for part in fc(input_stream):\n",
        "  print_part(part)"
      ],
      "metadata": {
        "id": "KIoq7uWqgqXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What's next?** Try it in your own code. Enabling function calling requires\n",
        "only a few extra lines of setup and well-documented docstrings."
      ],
      "metadata": {
        "id": "_In0o6XvgcVu"
      }
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      }
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
