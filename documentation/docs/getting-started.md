# Getting Started

This guide will walk you through setting up GenAI Processors and building your
first AI pipeline.

## Prerequisites

-   **Python 3.10+**
-   **Google AI Studio API Key** This is for the tutorials and the demos only.
    You can get an API for free
    [here](https://ai.google.dev/gemini-api/docs/api-key). Once you know more
    about the library, you will be able to use Gemma and other models available
    via Ollama or Transformers.

## Installation

Install the core library:

```bash
pip install genai-processors
```

If you plan to build real-time audio/video applications:

```bash
pip install "genai-processors[live]"
```

## Google API Configuration

We will use Gemini API extensively in our examples. Set your API key as an
environment variable for ease of use:

```bash
export GOOGLE_API_KEY="your_api_key_here"
```

## Your First Processor

Let's build a simple processor that accepts text input and generates a response
using Gemini. For this example, we use a Gemini API model.

```python
import asyncio
import os

from genai_processors import content_api
from genai_processors.core import genai_model

async def main():
    # Get your key from the environment variable
    api_key = os.environ.get("GOOGLE_API_KEY")

    # Initialize the model
    # You can also use 'vertex_ai' provider if configured
    model = genai_model.GenaiModel(
        model_name="gemini-3.0-flash",
        api_key=api_key,
    )

    # Process a single prompt
    output_stream = model(content_api.ContentStream(content=["Hello, GenAI!"]))

    # Iterate over the output stream
    async for part in output_stream:
        print(f"Received part: {part}\n")
        if part.text:
            print(f"Text: {part.text}\n")

if __name__ == "__main__":
    asyncio.run(main())
```

## Composition

The true power of GenAI Processors lies in composition. You can chain processors
using the `+` operator. More generally, you can combine processors in different
ways knowing they always follow the same simple signature: streams of parts
coming in and out. We recommend to use the `+` operator whenever possible to
benefit from the framework optimizations done under the hood: the processing of
parts is indeed implemented to minimize TTFT and concurrency is used extensively
whenever possible.

```python
from genai_processors import content_api
from genai_processors.core import preamble

# Create a processor that adds a prefix to any input.
system_prompt = preamble.Preamble(
    "You are a pirate styling assistant. Answer everything in pirate speak."
)

# Chain it with the model
pirate_bot = system_prompt + model

# Now the model will always act like a pirate
output_stream = pirate_bot(
    content_api.ContentStream(content=["What color matches blue?"])
    )
# Convenient method to gather textual content from a stream and to print it
# all at once. The drawback here is that you will only see the text once it is
# fully generated by the LLM (contrarily to the previous example that shows text
# as soon as it is generated).
print(await output_stream.text())
```

## Next Steps

Now that you have the basics, dive deeper into the core concepts:

-   **[Core Concepts](concepts/content-api.md)**: Learn about the unified data
    model and processor lifecycle.
-   **[Tutorials](tutorials/content-api.md)**: Specific guides for common tasks.
